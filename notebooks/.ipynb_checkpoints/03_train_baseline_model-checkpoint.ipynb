{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "64e33352-fb32-428e-9162-7c09f4542281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "\n",
    "BASE = '..'\n",
    "INPUT = os.path.join(BASE, 'data', 'Nova_pay_features.csv')\n",
    "MODELS = os.path.join(BASE, 'models')\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(INPUT, parse_dates=['timestamp'])\n",
    "df = df.dropna(subset=['timestamp', 'is_fraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "265ee398-6c68-4938-ad2e-7e7dad1bde25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11139 entries, 0 to 11138\n",
      "Data columns (total 32 columns):\n",
      " #   Column                     Non-Null Count  Dtype              \n",
      "---  ------                     --------------  -----              \n",
      " 0   transaction_id             11139 non-null  object             \n",
      " 1   customer_id                11139 non-null  object             \n",
      " 2   timestamp                  11139 non-null  datetime64[ns, UTC]\n",
      " 3   home_country               11139 non-null  object             \n",
      " 4   source_currency            11139 non-null  object             \n",
      " 5   dest_currency              11139 non-null  object             \n",
      " 6   channel                    11139 non-null  object             \n",
      " 7   amount_src                 11139 non-null  float64            \n",
      " 8   amount_usd                 11139 non-null  float64            \n",
      " 9   fee                        11139 non-null  float64            \n",
      " 10  exchange_rate_src_to_dest  11139 non-null  float64            \n",
      " 11  device_id                  11139 non-null  object             \n",
      " 12  new_device                 11139 non-null  bool               \n",
      " 13  ip_address                 11139 non-null  object             \n",
      " 14  ip_country                 11139 non-null  object             \n",
      " 15  location_mismatch          11139 non-null  bool               \n",
      " 16  ip_risk_score              11139 non-null  float64            \n",
      " 17  kyc_tier                   11139 non-null  object             \n",
      " 18  account_age_days           11139 non-null  int64              \n",
      " 19  device_trust_score         11139 non-null  float64            \n",
      " 20  chargeback_history_count   11139 non-null  int64              \n",
      " 21  risk_score_internal        11139 non-null  float64            \n",
      " 22  txn_velocity_1h            11139 non-null  int64              \n",
      " 23  txn_velocity_24h           11139 non-null  int64              \n",
      " 24  corridor_risk              11139 non-null  float64            \n",
      " 25  is_fraud                   11139 non-null  int64              \n",
      " 26  ip_country_missing         11139 non-null  bool               \n",
      " 27  hour                       11139 non-null  int64              \n",
      " 28  dayofweek                  11139 non-null  int64              \n",
      " 29  day_name                   11139 non-null  object             \n",
      " 30  hour_group                 11139 non-null  object             \n",
      " 31  risk_bin                   11129 non-null  object             \n",
      "dtypes: bool(3), datetime64[ns, UTC](1), float64(8), int64(7), object(13)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9d222dce-68b8-4670-ad58-c71e31707322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "categorical = ['home_country','source_currency','dest_currency','channel','kyc_tier','ip_country','new_device','location_mismatch', 'ip_country_missing']\n",
    "numeric = ['amount_src','amount_usd','fee','ip_risk_score','device_trust_score','account_age_days','txn_velocity_1h','txn_velocity_24h','corridor_risk','risk_score_internal', 'hour', 'dayofweek']\n",
    "for c in categorical:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cc897-9258-4cc0-949d-d0e6a981e7b3",
   "metadata": {},
   "source": [
    "### Data Spliting\n",
    "#### This code performs a time-based split of the dataset into train/validation/test using rolling time windows. It sorts the data, get the most recent timestamp and define cutoffs\n",
    "- training: data older than 270days **(>270DAYS)**\n",
    "- Validation: data from 270 --> 180days agao  **270-180DAYS)**\n",
    "- Test: most recent 180days  **(<180DAYS)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "71e3dfb0-1c9f-427d-bb30-7062559888bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 8817 | Validation: 1007 | Test: 1315\n",
      "----Fraud Rate in each set----\n",
      "Train set:      7.63%\n",
      "Validation set: 11.02%\n",
      "Test set:       15.89%\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values('timestamp')\n",
    "max_date = df['timestamp'].max()\n",
    "valid_start = max_date - pd.Timedelta(days=270)\n",
    "test_start = max_date - pd.Timedelta(days=180)\n",
    "\n",
    "train_df = df[df['timestamp'] < valid_start]\n",
    "valid_df = df[(df['timestamp'] >= valid_start) & (df['timestamp'] < test_start)]\n",
    "test_df = df[df['timestamp'] >= test_start]\n",
    "\n",
    "X_train = train_df[categorical + numeric]\n",
    "y_train = train_df['is_fraud'].astype(int).values\n",
    "X_valid = valid_df[categorical + numeric]\n",
    "y_valid = valid_df['is_fraud'].astype(int).values\n",
    "X_test = test_df[categorical + numeric]\n",
    "y_test = test_df['is_fraud'].astype(int).values\n",
    "\n",
    "print(f\"Training: {len(train_df)} | Validation: {len(valid_df)} | Test: {len(test_df)}\")\n",
    "\n",
    "print(\"----Fraud Rate in each set----\")\n",
    "print(f\"Train set:      {train_df['is_fraud'].mean() * 100:.2f}%\")\n",
    "print(f\"Validation set: {valid_df['is_fraud'].mean() * 100:.2f}%\")\n",
    "print(f\"Test set:       {test_df['is_fraud'].mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a1dbe-b91c-4214-b87c-dd4fadb0a81e",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e5a9b034-f386-4dc3-83d5-aa8fd651f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # One-hot econde categorical features\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
    "\n",
    "        # Standard scale numerical features\n",
    "        (\"num\", StandardScaler(with_mean=False), numeric),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837a5c1-4d5b-49b6-8ddd-51b23722b8e4",
   "metadata": {},
   "source": [
    "### Train LogisticRegression and Ramdom Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "43a9d257-38d4-4c8b-a2b3-ac31a462f392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"samples\": {\n",
      "    \"n_train\": 8817,\n",
      "    \"n_valid\": 1007,\n",
      "    \"n_test\": 1315\n",
      "  },\n",
      "  \"test_metrics\": {\n",
      "    \"logistic_regression\": {\n",
      "      \"threshold\": 0.5,\n",
      "      \"precision\": 0.7803921568627451,\n",
      "      \"recall\": 0.9521531100478469,\n",
      "      \"f1\": 0.8577586206896551,\n",
      "      \"roc_auc\": 0.9818562516763716,\n",
      "      \"false_positive_rate\": 0.05063291139240506,\n",
      "      \"confusion_matrix\": [\n",
      "        [\n",
      "          1050,\n",
      "          56\n",
      "        ],\n",
      "        [\n",
      "          10,\n",
      "          199\n",
      "        ]\n",
      "      ]\n",
      "    },\n",
      "    \"random_forest\": {\n",
      "      \"threshold\": 0.5,\n",
      "      \"precision\": 0.9949238578680203,\n",
      "      \"recall\": 0.937799043062201,\n",
      "      \"f1\": 0.9655172413793104,\n",
      "      \"roc_auc\": 0.9791243067392301,\n",
      "      \"false_positive_rate\": 0.0009041591320072332,\n",
      "      \"confusion_matrix\": [\n",
      "        [\n",
      "          1105,\n",
      "          1\n",
      "        ],\n",
      "        [\n",
      "          13,\n",
      "          196\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "log_reg = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"model\", LogisticRegression(max_iter=200, class_weight=\"balanced\", n_jobs=None))\n",
    "])\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_prob_log_reg = log_reg.predict_proba(X_test)[:,1]\n",
    "\n",
    "rf = Pipeline(steps=[\n",
    "              (\"prep\", preprocessor),\n",
    "              (\"model\", RandomForestClassifier(\n",
    "                  n_estimators=300,\n",
    "                  random_state=42,\n",
    "                  class_weight=\"balanced_subsample\",\n",
    "                  n_jobs=-1\n",
    "              ))\n",
    "])\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_prob_rf = rf.predict_proba(X_test)[:,1]\n",
    "\n",
    "def metrics(y_true, y_prob, thr=0.5):\n",
    "    y_pred = (y_prob >= thr).astype(int)\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    roc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else float(\"nan\")\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) else 0.0\n",
    "\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"precision\": float(p),\n",
    "        \"recall\": float(r),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(roc),\n",
    "        \"false_positive_rate\": float(fpr),\n",
    "        \"confusion_matrix\": [[int(tn), int(fp)], [int(fn), int(tp)]],\n",
    "    }\n",
    "\n",
    "res = {\n",
    "    \"samples\": {\n",
    "        \"n_train\": int(len(X_train)),\n",
    "        \"n_valid\": int(len(X_valid)),\n",
    "        \"n_test\": int(len(X_test)),\n",
    "    },\n",
    "    \"test_metrics\": {\n",
    "        \"logistic_regression\": metrics(y_test, y_prob_log_reg, thr=0.5),\n",
    "        \"random_forest\": metrics(y_test, y_prob_rf, thr=0.5),\n",
    "    },\n",
    "}\n",
    "print(json.dumps(res, indent=2))\n",
    "\n",
    "os.makedirs(MODELS, exist_ok=True)\n",
    "joblib.dump(log_reg, os.path.join(MODELS, 'model_log_reg.joblib'))\n",
    "joblib.dump(rf, os.path.join(MODELS, 'model_rf.joblib'))\n",
    "with open(os.path.join(MODELS, 'metrics_baseline.json'),'w',encoding='utf-8') as f:\n",
    "    json.dump(res, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b57631-9579-4947-b4c7-0db4bccf6ffb",
   "metadata": {},
   "source": [
    "#### Both models show excellent discriminatory power (ROC-AUC ‚âà 0.98).\n",
    "#### Random Forest achieves near-zero false positives at default threshold, while Logistic Regression achieves slightly higher recall at the cost of higher customer friction.\n",
    "#### Final model selection should be based on threshold optimization under a business-defined false-positive constraint.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "95d3f0bb-54f0-498a-bea4-1d46a0d0d883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Logistic Regression ‚Äì Test Set Performance\n",
      "\n",
      "Recall:              95.2%  ‚Üí missed 10 frauds\n",
      "Precision:           78.0%\n",
      "False Positive Rate: 5.06% (56 false positives)\n",
      "ROC-AUC:             0.982\n",
      "-------------------------------------------------------\n",
      "\n",
      "üü¢ Random Forest ‚Äì Test Set Performance\n",
      "\n",
      "Recall:              93.8%  ‚Üí missed 13 frauds\n",
      "Precision:           99.5%\n",
      "False Positive Rate: 0.09% (1 false positives)\n",
      "ROC-AUC:             0.979\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_model_summary(model_name, metrics_dict):\n",
    "    print(f\"üü¢ {model_name} ‚Äì Test Set Performance\\n\")\n",
    "    print(f\"Recall:              {metrics_dict['recall']*100:.1f}%  \"\n",
    "          f\"‚Üí missed {metrics_dict['confusion_matrix'][1][0]} frauds\")\n",
    "    print(f\"Precision:           {metrics_dict['precision']*100:.1f}%\")\n",
    "    print(f\"False Positive Rate: {metrics_dict['false_positive_rate']*100:.2f}% \"\n",
    "          f\"({metrics_dict['confusion_matrix'][0][1]} false positives)\")\n",
    "    print(f\"ROC-AUC:             {metrics_dict['roc_auc']:.3f}\")\n",
    "    print(\"-\" * 55 + \"\\n\")\n",
    "\n",
    "# Extract metrics\n",
    "lr_metrics = res[\"test_metrics\"][\"logistic_regression\"]\n",
    "rf_metrics = res[\"test_metrics\"][\"random_forest\"]\n",
    "\n",
    "# Print summaries\n",
    "print_model_summary(\"Logistic Regression\", lr_metrics)\n",
    "print_model_summary(\"Random Forest\", rf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf01e8e-52da-42d2-b451-c4011c946b28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pycaret310_fixed]",
   "language": "python",
   "name": "conda-env-pycaret310_fixed-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
